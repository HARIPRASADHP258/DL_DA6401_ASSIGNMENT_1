# -*- coding: utf-8 -*-
"""dltest1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18qnd4vQ5f_pOJ7sMpEh2ocrlPR8tHK2h
"""

import numpy as np
from keras.datasets import fashion_mnist
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import time
import argparse

# Activation Functions and Derivatives
def logistic(x):
    return 1 / (1 + np.exp(-x))

def logistic_grad(a):
    return a * (1 - a)

def hyperbolic(x):
    return np.tanh(x)

def hyperbolic_grad(a):
    return 1 - np.tanh(a) ** 2

def relu(x):
    return np.maximum(0, x)

def relu_grad(a):
    return np.where(a > 0, 1, 0)

def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

# Loss Function
def compute_loss(y_target, y_predicted):
    m = y_target.shape[0]
    return -np.sum(y_target * np.log(y_predicted + 1e-8)) / m

def accuracy(y_target, y_predicted):
    return np.mean(np.argmax(y_target, axis=1) == np.argmax(y_predicted, axis=1))

# Weight Initialization
def init_params(input_dim, output_dim, method="random"):
  #Initialize weight matrices based on the chosen method

    if method == "random":
        return np.random.randn(input_dim, output_dim) * 0.01
    elif method == "xavier":
        return np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / (input_dim + output_dim))
    elif method == "random_normal":
        return np.random.normal(loc=0.0, scale=0.01, size=(input_dim, output_dim))
    elif method == "xavier_uniform":
        limit = np.sqrt(6.0 / (input_dim + output_dim))
        return np.random.uniform(low=-limit, high=limit, size=(input_dim, output_dim))
    elif method == "xavier_normal":
        return np.random.normal(loc=0.0, scale=np.sqrt(2.0 / (input_dim + output_dim)), size=(input_dim, output_dim))

# Base Optimizer Class
class GradientUpdater:
  #Base class for gradient-based optimizers
    def __init__(self, lr, weight_decay=0.0):
        self.lr = lr
        self.weight_decay = weight_decay

    def apply_gradients(self, params, grads):
        raise NotImplementedError

# Optimizers
class BasicUpdater(GradientUpdater):
#Applies standard gradient descent with weight decay
    def apply_gradients(self, params, grads):
        return params - self.lr * (grads + self.weight_decay * params)

class MomentumUpdater(GradientUpdater):
    def __init__(self, lr, beta=0.9, weight_decay=0.0):
        super().__init__(lr, weight_decay)
        self.beta = beta # Momentum factor
        self.velocity = None  # Stores previous gradients

    def apply_gradients(self, params, grads):
        if self.velocity is None:
            self.velocity = np.zeros_like(params)   # Initialize velocity
        self.velocity = self.beta * self.velocity + (1 - self.beta) * grads
        return params - self.lr * (self.velocity + self.weight_decay * params)

class NesterovUpdater(MomentumUpdater):
    def apply_gradients(self, params, grads):
        if self.velocity is None:
            self.velocity = np.zeros_like(params)
        lookahead = params - self.beta * self.velocity # Lookahead step
        self.velocity = self.beta * self.velocity + self.lr * grads
        return lookahead - self.velocity - self.lr * self.weight_decay * params

class RMSUpdater(GradientUpdater):
    def __init__(self, lr, beta=0.99, epsilon=1e-8, weight_decay=0.0):
        super().__init__(lr, weight_decay)
        self.beta = beta # Decay rate for squared gradient accumulation
        self.epsilon = epsilon #Small constant for numerical stability
        self.squared_avg = None # Stores moving average of squared gradients

    def apply_gradients(self, params, grads):
        if self.squared_avg is None:
            self.squared_avg = np.zeros_like(params)
        self.squared_avg = self.beta * self.squared_avg + (1 - self.beta) * grads ** 2
        return params - self.lr * grads / (np.sqrt(self.squared_avg) + self.epsilon) - self.lr * self.weight_decay * params

class AdaptiveUpdater(GradientUpdater):
  #Adaptive moment estimation (Adam) optimizer
    def __init__(self, lr, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.0):
        super().__init__(lr, weight_decay)
        self.beta1 = beta1  # First moment decay rate
        self.beta2 = beta2  # Second moment decay rate
        self.epsilon = epsilon
        self.m = None # First moment estimate
        self.v = None # Second moment estimate
        self.t = 0  # Time step

    def apply_gradients(self, params, grads):
        if self.m is None:
            self.m = np.zeros_like(params)
            self.v = np.zeros_like(params)

        self.t += 1
        self.m = self.beta1 * self.m + (1 - self.beta1) * grads   # Compute biased first moment estimate
        self.v = self.beta2 * self.v + (1 - self.beta2) * grads ** 2  # Compute biased second moment estimate

        # Bias correction
        m_hat = self.m / (1 - self.beta1 ** self.t)
        v_hat = self.v / (1 - self.beta2 ** self.t)

        # Update parameters
        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon) - self.lr * self.weight_decay * params

class NesterovAdaptiveUpdater(AdaptiveUpdater):
   #Adam optimizer with Nesterov acceleration for faster convergence
    def apply_gradients(self, params, grads):
        if self.m is None:
            self.m = np.zeros_like(params)
            self.v = np.zeros_like(params)

        self.t += 1
        self.m = self.beta1 * self.m + (1 - self.beta1) * grads
        self.v = self.beta2 * self.v + (1 - self.beta2) * grads ** 2

        # Bias correction
        m_hat = self.m / (1 - self.beta1 ** self.t)
        v_hat = self.v / (1 - self.beta2 ** self.t)

        # Compute Nesterov momentum term
        m_nesterov = self.beta1 * m_hat + (1 - self.beta1) * grads

        return params - self.lr * m_nesterov / (np.sqrt(v_hat) + self.epsilon)

# Neural Network Class
class MultiLayerModel:
    def __init__(self, input_dim, layer_sizes, activations, optimizer="adam", lr=0.001, weight_init="xavier", weight_decay=0.0):
        self.layer_sizes = layer_sizes

        #activation and its derivatives
        self.activations = [relu if act == "relu" else hyperbolic if act == "tanh" else logistic for act in activations]
        self.activation_grads = [relu_grad if act == "relu" else hyperbolic_grad if act == "tanh" else logistic_grad for act in activations]
        #weights and biases initialization
        self.weights = [init_params(input_dim, layer_sizes[0], weight_init)] + [init_params(layer_sizes[i], layer_sizes[i+1], weight_init) for i in range(len(layer_sizes)-1)]
        self.biases = [np.zeros((1, size)) for size in layer_sizes]


        self.loss_history = [] # storage for loss
        self.learning_rate = lr
        self.weight_decay = weight_decay #L2 regularisor

        print("\nSanity Check: Bias Shapes")
        for i, b in enumerate(self.biases):
            print(f"Layer {i+1} Bias Shape: {b.shape}")


        optimizers = {
            "sgd": BasicUpdater,
            "momentum": MomentumUpdater,
            "nag": NesterovUpdater,
            "rmsprop": RMSUpdater,
            "adam": AdaptiveUpdater,
            "nadam": NesterovAdaptiveUpdater
            }


        self.optimizers = [optimizers[optimizer](lr, weight_decay=weight_decay) for _ in layer_sizes]
        self.train_loss = []
        self.val_loss = []
        self.train_acc = []
        self.val_acc = []



    def forward_pass(self, X):
        self.layer_outputs = [X]

        for i in range(len(self.layer_sizes)):
            X = np.dot(X, self.weights[i]) + self.biases[i] #z=X*W+b
            X = softmax(X) if i == len(self.layer_sizes) - 1 else self.activations[i](X)
            self.layer_outputs.append(X)
        return X

    def backward_pass(self, y_target):
        output_grad = self.layer_outputs[-1] - y_target  # Compute the gradient of the loss of output layer
        grads = [output_grad]

        # Backpropagate the error through the hidden layers
        for i in range(len(self.layer_sizes) - 1, 0, -1):
            output_grad = np.dot(output_grad, self.weights[i].T) * self.activation_grads[i - 1](self.layer_outputs[i])
            grads.insert(0, output_grad)


      # Update weights and biases using the computed gradients
        for i in range(len(self.layer_sizes)):
            self.weights[i] = self.optimizers[i].apply_gradients(self.weights[i], np.dot(self.layer_outputs[i].T, grads[i]))
            self.biases[i] -= self.learning_rate * np.sum(grads[i], axis=0, keepdims=True)

    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=32, grad_type="mini-batch"):
        num_samples = X_train.shape[0]


      # Determine batch size based on gradient update type
        if grad_type == "stochastic":
            batch_size = 1
        elif grad_type == "vanilla":
            batch_size = num_samples
        elif grad_type == "mini-batch":
            batch_size = min(batch_size, num_samples)
        else:
            raise ValueError("Invalid grad_type. Choose from 'stochastic', 'vanilla', or 'mini-batch'.")


        for epoch in range(epochs):
          # Shuffle data to avoid training bias
            indices = np.arange(num_samples)
            np.random.shuffle(indices)
            X_train = X_train[indices]
            y_train = y_train[indices]

            epoch_loss = 0
            num_batches = 0

            # Iterate through the dataset in mini-batches
            for i in range(0, num_samples, batch_size):
                X_batch, y_batch = X_train[i:i + batch_size], y_train[i:i + batch_size]
                y_pred = self.forward_pass(X_batch)

                # Compute loss for the batch
                loss = compute_loss(y_batch, y_pred)
                epoch_loss += loss
                num_batches += 1

                # Perform backpropagation to update weights
                self.backward_pass(y_batch)

            avg_loss = epoch_loss / num_batches # average loss since batches
            train_acc = accuracy(y_train, self.forward_pass(X_train))
            val_acc = accuracy(y_val, self.forward_pass(X_val))
            val_loss = compute_loss(y_val, self.forward_pass(X_val))

            # appending for plot metrics visualization
            self.train_loss.append(avg_loss)
            self.train_acc.append(train_acc)
            self.val_loss.append(val_loss)
            self.val_acc.append(val_acc)
            self.loss_history.append(avg_loss)
            print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Train Acc: {train_acc:.4f} - Val Acc: {val_acc:.4f} - Val Loss: {val_loss:.4f}")



    def evaluate(self, X_test, y_test):
        y_pred = self.forward_pass(X_test) # Perform forward pass on test data
        test_acc = accuracy(y_test, y_pred) # Compute test accuracy
        print(f"Test Accuracy: {test_acc:.4f}")
        return test_acc

    def plot_metrics(self):
        plt.figure(figsize=(12, 5))

        # Plot training and validation loss over epochs
        plt.subplot(1, 2, 1)
        plt.plot(self.train_loss, label='Training Loss')
        plt.plot(self.val_loss, label='Validation Loss')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title('Training and Validation Loss')
        plt.legend()

        # Plot training and validation accuracy over epochs
        plt.subplot(1, 2, 2)
        plt.plot(self.train_acc, label='Training Accuracy')
        plt.plot(self.val_acc, label='Validation Accuracy')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.title('Training and Validation Accuracy')
        plt.legend()

        plt.show()



def main():
    # Parse command-line arguments
    parser = argparse.ArgumentParser(description="Train a neural network on Fashion MNIST.")
    parser.add_argument("--epochs", type=int, default=5, help="Number of epochs to train.")
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size for training.")
    parser.add_argument("--loss", type=str, default="squared_error", help="Loss function to use.")
    parser.add_argument("--optimizer", type=str, default="adam", help="Optimizer to use.")
    parser.add_argument("--learning_rate", type=float, default=0.001, help="Learning rate for the optimizer.")
    parser.add_argument("--activation", type=str, default="relu", help="Activation function to use.")
    parser.add_argument("--weight_init", type=str, default="xavier", help="Weight initialization method.")
    parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay (L2 regularization) value.")
    parser.add_argument("--hidden_layers", type=int, default=3, help="Number of hidden layers.")
    parser.add_argument("--layer_size", type=int, default=128, help="Number of neurons in each hidden layer.")
    args = parser.parse_args()

    # Load and preprocess the dataset
    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
    X_train, X_test = X_train.reshape(-1, 784) / 255.0, X_test.reshape(-1, 784) / 255.0
    y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

    # Define model architecture
    layer_sizes = [args.layer_size] * args.hidden_layers  # Hidden layers
    activations = [args.activation] * args.hidden_layers  # Activation functions

    # Initialize the model
    model = MultiLayerModel(
        input_dim=784,
        layer_sizes=layer_sizes + [10],  # Output layer
        activations=activations + ['linear'],  # Linear activation for output layer
        optimizer=args.optimizer,
        lr=args.learning_rate,
        weight_init=args.weight_init,
        weight_decay=args.weight_decay
    )

    # Train the model
    model.train(X_train, y_train, X_val, y_val, epochs=args.epochs, batch_size=args.batch_size)

    # Evaluate the model
    test_acc = model.evaluate(X_test, y_test)
    print(f"Test Accuracy: {test_acc:.4f}")


if __name__ == "__main__":
    main()